
![alt RGDORA](https://github.com/NahinAlam001/meme/blob/main/RGDORA.png?raw=true)
---
# ğŸŒ RGDORA: Retrieval-Guided Domain Robust Architecture
### Bengali Hateful Meme Detection System ğŸ‡§ğŸ‡©

> **RGDORA** is a domain-robust multimodal architecture for **Bengali hateful meme detection**, integrating retrieval-guided contrastive learning, class imbalance handling, and rationale generation.

---

## ğŸ”¥ Features

- âœ… Full **checkpoint resumability**
- âœ… **Early stopping** with validation monitoring
- âœ… Advanced **class imbalance handling** (Focal Loss, Class-Balanced Loss, Balanced Sampling)
- âœ… **Mixed precision** training (AMP)
- âœ… **FAISS-based retrieval-guided** contrastive learning
- âœ… **Rationale generation** via BART
- âœ… Comprehensive **metrics & evaluation** suite

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ dataset.py             # Dataset loader for BHM
â”œâ”€â”€ models.py              # RGDORA and NegativeGenerator
â”œâ”€â”€ trainer.py             # Main training system
â”œâ”€â”€ metrics.py             # Metrics tracking
â”œâ”€â”€ early_stopping.py      # Early stopping handler
â”œâ”€â”€ checkpoint_manager.py  # Checkpoint management
â”œâ”€â”€ faiss_manager.py       # FAISS index management
â”œâ”€â”€ imbalance_handler.py   # Loss functions and samplers
â”œâ”€â”€ main.py                # Entry point
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ setup.sh               # Installation script
â””â”€â”€ README.md              # This file

```
---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Setup Environment

```bash
chmod +x setup.sh
./setup.sh
```

Installs all dependencies and downloads the **Bengali Hateful Meme (BHM)** dataset automatically.

---

### 2ï¸âƒ£ Train Models

Train **Task 1 (Binary: Hate vs Non-Hate)**

```bash
python main.py --task task1
```

Train **Task 2 (Multi-class: TI, TC, TO, TS)**

```bash
python main.py --task task2
```

Train **both tasks sequentially**

```bash
python main.py --task both
```

---

### 3ï¸âƒ£ Resume Training

Automatically resumes from the latest checkpoint:

```bash
python main.py --task task1
```

---

## ğŸ“Š Dataset: Bengali Hateful Meme (BHM)

### **Task 1: Binary Classification**

| Class     | Label | Count     | Ratio    |
| :-------- | :---- | :-------- | :------- |
| Hate      | 1     | 3,418     | 59.4%    |
| Non-Hate  | 0     | 2,340     | 40.6%    |
| **Total** | â€”     | **5,758** | 1.72 : 1 |

### **Task 2: Multi-class Classification**

| Class                    | Label | Count     | Ratio  |
| :----------------------- | :---- | :-------- | :----- |
| Target Individual (TI)   | 0     | 1,623     | 76.7%  |
| Target Community (TC)    | 1     | 249       | 11.8%  |
| Target Organization (TO) | 2     | 160       | 7.6%   |
| Target Society (TS)      | 3     | 85        | 4.0%   |
| **Total**                | â€”     | **2,117** | 19 : 1 |

---

## âš™ï¸ Configuration

You can edit `config.py` or override parameters via CLI.

```python
config = RGDORAConfig(
    task="task1",                # "task1" or "task2"
    batch_size=8,
    epochs=20,
    learning_rate=2e-5,
    loss_type="focal",           # "focal", "class_balanced", "weighted_ce"
    use_balanced_sampler=True,
    patience=5,                  # Early stopping patience
    monitor_metric="val_macro_f1"
)
```

---

## ğŸ§© Key Components

### ğŸ¯ Class Imbalance Handling

* **Focal Loss:** Down-weights easy examples
* **Class-Balanced Loss:** Uses effective sample numbers
* **Balanced Batch Sampler:** Ensures equal representation
* **Weighted Random Sampler:** Oversamples minorities

### âš–ï¸ Training Stability

* Gradient clipping
* Mixed precision (AMP)
* NaN-safe batch skipping
* Warmup + linear LR scheduling

### ğŸ“ˆ Evaluation Metrics

* Accuracy & Balanced Accuracy
* Macro / Weighted F1, Precision, Recall
* Matthews Correlation Coefficient (MCC)
* Cohenâ€™s Kappa
* ROC-AUC & PR-AUC
* Per-class metrics

---

## ğŸ”¬ Model Architecture

**RGDORA Components**

| Component             | Description                           |
| --------------------- | ------------------------------------- |
| ğŸ–¼ï¸ Visual Encoder    | CLIP (ViT-B/32)                       |
| ğŸ”¤ Text Encoder       | XGLM-564M (Multilingual)              |
| âš™ï¸ Fusion             | Feature Interaction Matrix (FIM)      |
| ğŸ§  Classifier         | 2-layer MLP + LayerNorm               |
| ğŸ§¾ Rationale Decoder  | BART-base                             |
| ğŸ’£ Negative Generator | 3-layer MLP for adversarial negatives |

**Training Objectives:**

* Classification Loss (Focal or Class-Balanced)
* Retrieval-Guided Contrastive Loss (RGCL)
* Optional Rationale Generation Loss

---

## ğŸ§  Performance Tips

### ğŸŸ¦ Task 1 (Binary)

* `loss_type="focal"` with `focal_alpha=0.25`
* `sampler_alpha=0.7` (moderate balancing)
* Monitor `val_macro_f1`

### ğŸŸ¥ Task 2 (Multi-class)

* `loss_type="class_balanced"` with `cb_beta=0.9999`
* `sampler_alpha=0.9` (aggressive balancing)
* `patience=7`, `learning_rate=1.5e-5`

---

## ğŸ“‚ Outputs

All logs, checkpoints, and metrics are automatically saved.

| Task   | Logs Directory | Checkpoints Directory | Best Model      |
| ------ | -------------- | --------------------- | --------------- |
| Task 1 | `logs_task1/`  | `checkpoints_task1/`  | `best_model.pt` |
| Task 2 | `logs_task2/`  | `checkpoints_task2/`  | `best_model.pt` |

Monitor training live:

```bash
tail -f logs_task1/training.log
```

---

## ğŸ§¾ Citation

If you use this code, please cite:

```bibtex
@misc{rgdora2025,
  title={RGDORA: Retrieval-Guided Domain Robust Architecture for Bengali Hateful Meme Detection},
  author={Md. Nahin Alam},
  year={2025}
}
```

---

## ğŸ¤ Contributing

Contributions are welcome!
Please open an issue or submit a pull request.

---

## ğŸ“œ License

**MIT License** â€” see [`LICENSE`](./LICENSE) for details.

---

## ğŸ§© Usage Summary

```bash
# 1. Setup (one-time)
chmod +x setup.sh
./setup.sh

# 2. Train Task 1 (Binary)
python main.py --task task1

# 3. Train Task 2 (Multi-class)
python main.py --task task2

# 4. Train both tasks
python main.py --task both

# 5. Monitor training
tail -f logs_task1/training.log

# 6. Resume interrupted training (auto)
python main.py --task task1
```

---

> ğŸ’¡ **Tip:** RGDORA is fully modular â€” plug in any dataset or encoder by extending `dataset.py` and `models.py`.

---

<p align="center">
  <b>Developed with â¤ï¸ by <a href="https://github.com/nahinalam">Md. Nahin Alam</a></b><br>
  <i>For robust Bengali multimodal hate speech understanding.</i>
</p>
